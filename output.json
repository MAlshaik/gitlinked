[
    {
        "Repository": "2023-SIAM-DS-TDA-Minitutorial",
        "Languages": "Jupyter Notebook, CSS, ",
        "Description": "Uzair",
        "README": "#  Topological Signal Processing for Dynamical Systems\n\nThis repository contains the content for a [minitutorial](https://www.siam.org/conferences/cm/program/minitutorials/ds23-minitutorials) for the [2023 SIAM Conference on Applications of Dynamical Systems (DS23)](https://www.siam.org/conferences/cm/conference/ds23). **If you're planning to attend the minitutorial, please see [the instructions below](#accessing-the-content) for some setup that will help get us up and running faster.**\n\n- [Part I Information](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75586): Mon May 15, 1:20-3:20PM \n- [Part II Information](https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=77160): Mon May 15 4:45-6:45PM\n- Both sessions in Lloyd Center Ballroom - 1st Level\n\nContributors to this minitutorial are:\n\n- [Firas Khasawneh](https://firaskhasawneh.com)\n- [Elizabeth Munch](https://elizabethmunch.com)\n- [Max Chumley](https://www.maxchumley.com)\n- [Danielle Barnes](https://github.com/barnesd8) \n- [Sunia Tanweer](https://stanweer1.github.io)\n\n## Abstract\n\nPersistent homology, the flagship method of topological data analysis (TDA), can be used to provide a quantitative summary of the shape of data.  There is now extensive work using the following pipeline. Start with a realization of a dynamical system, perhaps after reconstruction from time series data. Treat this data as a discrete metric space, and construct a filtration of a simplicial complex that encodes information about the shape.  Then, use persistent homology to determine when structures appear and disappear in this filtration, which can in turn be used as input to tools such as machine learning models. \n\nThis mini-tutorial will cover the basics of each piece of the aforementioned pipeline, as well as show more recently studied modifications that show a great deal of promise for future analysis of dynamical systems with TDA. This tutorial will be run using active learning, with participants coding examples in python in provided Jupyter notebooks, making extensive use of the open-source package [teaspoon](https://github.com/TeaspoonTDA/teaspoon). \n\n![](figures/big_picture.png)\n\n## Schedule \n\n\n### Session 1: Persistent Homology Basics\n\n0. Welcome Remarks and Introduction (5 minutes)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-0-Welcome.ipynb)\n   - Presenter: Firas Khasawneh & Liz Munch\n1. Simplicial complexes & Homology (30 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-1-SimplicialCpx_Homology.ipynb)\n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-1-Wkst-SimplicialCpx_Homology.ipynb) \n   - Presenter: Liz Munch\n2. Persistent homology (30 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-2-PersistentHomology.ipynb) \n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-2-Wkst-PersistentHomology.ipynb) \n   - Presenter: Liz Munch\n3. Persistence Pipelines: Rips & Images (30 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-3-PersistencePipelines.ipynb)\n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-3-PersistencePipelines.ipynb) \n   - Presenter: Firas Khasawneh \n4. Distances on persistence diagrams & Vectorization (10 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/1-4-DistancesAndVectorization.ipynb)\n   - Presenter: Danielle Barnes\n5. Session 1 Wrap up (5 minutes)\n   - Slides\n   - Presenter: Firas Khasawneh & Liz Munch\n   \n### Session 2: Using Persistence for Signal Processing\n\n0. Welcome Remarks and Introduction (5 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-0-Welcome.ipynb)\n   - Presenter: Firas Khasawneh & Liz Munch\n1. Embedding of Time series (40 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-1-Embedding.ipynb)\n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-1-Wkst-Embedding.ipynb)\n   - Presenter: Max Chumley\n2. Graph representations of time series (30 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-2-GraphTimeSeries.ipynb)\n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-2-Wkst-GraphTimeSeries.ipynb)\n   - Presenter: Max Chumley\n3. Crocker plots (20 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-3-CrockerPlots.ipynb) \n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-3-Wkst-CrockerPlots.ipynb)\n   - Presenter: Sunia Tanweer\n4. Identifying P-bifurcations in Stochastic Systems (10 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-4-Stochastics.ipynb)\n   - [Jupyter notebook](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-4-Wkst-Stochastics.ipynb)\n   - Presenter: Sunia Tanweer\n5. Session 2 Wrap up (5 min)\n   - [Slides](https://github.com/TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial/blob/main/2-5-Wrapup.ipynb)\n   - Presenter: Firas Khasawneh & Liz Munch\n\n## Accessing the content\n\nThis minitutorial will be run in an active learning style. We will present basic introductions to the concepts followed by time to try out examples provided in the Jupyter notebooks in this GitHub repository.  We highly recommend that you bring a laptop to the workshop. If at all possible, we also recommend following the below instructions in advance in case of issues. We will have two ways to access the content during the workshop itself:\n\n- Locally installed on your machine. You will need to clone the repository and get the code installed in advance. Unfortunately we cannot provide much in the way of install help due to the size and nature of the workshop. So, if local install doesn't work, we have provided.....\n- Remote servers where you will be able to access all the material during the workshop. Please note that these servers will only be active during the workshop itself. \n\nNote that the remote version will require a GitHub account, so we recommend making one in advance in case of issues, even if planning to run the code locally.\n\n### Local Install Instructions\n\n#### Teaspoon\n\nTo install [teaspoon](https://github.com/TeaspoonTDA/teaspoon) locally, please follow the directions provided in [Getting Started](https://teaspoontda.github.io/teaspoon/installation.html#).  There are two system dependencies:\n-  [cmake](https://cmake.org/install/) and \n-  [boost for Unix](https://www.boost.org/doc/libs/1_66_0/more/getting_started/unix-variants.html) or [boost for Windows](https://www.boost.org/doc/libs/1_62_0/more/getting_started/windows.html).  \n\nWhile we make every effort to ensure compatibility across operating systems, we cannot guarantee you will not encounter install issues.\n\nIn addition to teaspoon, `RISE` and `giotto-tda` are required for use of the slide decks and specific portions of the minitutorial.  Once system dependencies are installed, to install the required python packages you should be able to run:\n\n``` \n\npip install teaspoon\npip install RISE\npip install giotto-tda\n```\n\n#### Clone the repo\n\nYou will want to clone this repository to be of use in your local machine. We list two options below for cloning the repo.\n\n##### Clone using GiHub Desktop\nThe first option to clone the repository is via [GitHub Desktop](https://desktop.github.com/), which is available for Windows and macOS. After installing the app, go to *File>Clone Repository*, click the <kbd>URL</kbd> tab, and enter the repository's address. Choose where the local path for the repository, and hit <kbd>Clone</kbd>.  \n\n##### Clone using Git\nAssuming you already have ssh setup for GitHub (see [here](https://docs.github.com/en/authentication/connecting-to-github-with-ssh) if you need additional directions) the repository can be cloned with the following command.\n\n```\ngit clone git@github.com:TeaspoonTDA/2023-SIAM-DS-TDA-Minitutorial.git\n```\n\n### Remote Connection Instructions\n\nTo connect remotely, you will need to setup a GitHub account and then at the time of the workshop, connect to a remote server. Then all code and workbooks will be run directly via web browser. \n\n#### Github account creation\n\nPlease follow the [GitHub signup instructions](https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-account).\n\n#### Server Login Information\n\n**Note:** These links and instructions will not be updated until the minitutorial begins.\n\nTo run your computations on a provided Jupyter Server, login by choosing one of the URL's below as instructed:\n\n- [Server 1](https://9g72.short.gy/server1)\n- [Server 2](https://9g72.short.gy/server2)\n\nYou must use the same URL to login for the full minitutorial since these are specific servers where your work will be located.  At the end of the conference, you will also need to download your work as the servers will not be available after the conference ends.\n\n# Funding \n\nWe are grateful to the following agencies for their support of this project. \n\n- The Air Force Office of Scientific Research under Award FA9550-22-1-0007.\n- The National Science Foundation under awards CCF-2106578, CCF-1907591, and CCF-2142713. \n- Michigan State University \n- The Society for Industrial and Applied Mathematics\n"
    },
    {
        "Repository": "cmse381-F23",
        "Languages": "Jupyter Notebook, ",
        "Description": "Fork of course content for CMSE381 - Fall 2023, taught my Dr Liz Munch",
        "README": "Fork of this repo to work on lecture notebooks\n\n# CMSE381 - Fall 2023\n\n## Dr Elizabeth Munch\n\nThis course has the contents for the CMSE381 course in Fall 2023. Check back here for class slides and jupyter notebooks.\n"
    },
    {
        "Repository": "docker-tutorial",
        "Languages": "Python, Dockerfile, ",
        "Description": "learning how to use docker",
        "README": ""
    },
    {
        "Repository": "documents",
        "Languages": "",
        "Description": "includes my resume",
        "README": ""
    },
    {
        "Repository": "HoloLens-Streamlit",
        "Languages": "Python, ",
        "Description": "",
        "README": "# HoloLens-Streamlit\n\nThe streamlit app for our 2023 HackUIowa Project: Hologlass.\n\nReceive real time transcripts of your conversations and review their summaries online\n"
    },
    {
        "Repository": "InvokeAI",
        "Languages": "Jupyter Notebook, Python, JavaScript, HTML, Shell, CSS, ",
        "Description": "This version of CompVis/stable-diffusion features an interactive command-line script that combines text2img and img2img functionality in a \"dream bot\" style interface, a WebGUI, and multiple features and other enhancements.",
        "README": "<h1 align='center'><b>InvokeAI: A Stable Diffusion Toolkit</b></h1>\n\n<p align='center'>\n<img src=\"docs/assets/logo.png\"/>\n</p>\n\n<p align=\"center\">\n    <img src=\"https://img.shields.io/github/last-commit/invoke-ai/InvokeAI?logo=Python&logoColor=green&style=for-the-badge\" alt=\"last-commit\"/>\n    <img src=\"https://img.shields.io/github/stars/invoke-ai/InvokeAI?logo=GitHub&style=for-the-badge\" alt=\"stars\"/>\n    <br>\n    <img src=\"https://img.shields.io/github/issues/invoke-ai/InvokeAI?logo=GitHub&style=for-the-badge\" alt=\"issues\"/>\n    <img src=\"https://img.shields.io/github/issues-pr/invoke-ai/InvokeAI?logo=GitHub&style=for-the-badge\" alt=\"pull-requests\"/>\n</p>\n\nThis is a fork of\n[CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion),\nthe open source text-to-image generator. It provides a streamlined\nprocess with various new features and options to aid the image\ngeneration process. It runs on Windows, Mac and Linux machines,\nand runs on GPU cards with as little as 4 GB or RAM.\n\n_Note: This fork is rapidly evolving. Please use the\n[Issues](https://github.com/invoke-ai/InvokeAI/issues) tab to\nreport bugs and make feature requests. Be sure to use the provided\ntemplates. They will help aid diagnose issues faster._\n\n_This repository was formally known as /stable-diffusion_\n\n# **Table of Contents**\n\n1. [Installation](#installation)\n2. [Major Features](#features)\n3. [Changelog](#latest-changes)\n4. [Troubleshooting](#troubleshooting)\n5. [Contributing](#contributing)\n6. [Support](#support)\n\n# Installation\n\nThis fork is supported across multiple platforms. You can find individual installation instructions below.\n\n- ## [Linux](docs/installation/INSTALL_LINUX.md)\n- ## [Windows](docs/installation/INSTALL_WINDOWS.md)\n- ## [Macintosh](docs/installation/INSTALL_MAC.md)\n\n## **Hardware Requirements**\n\n**System**\n\nYou wil need one of the following:\n\n- An NVIDIA-based graphics card with 4 GB or more VRAM memory.\n- An Apple computer with an M1 chip.\n\n**Memory**\n\n- At least 12 GB Main Memory RAM.\n\n**Disk**\n\n- At least 6 GB of free disk space for the machine learning model, Python, and all its dependencies.\n\n**Note**\n\nIf you are have a Nvidia 10xx series card (e.g. the 1080ti), please\nrun the dream script in full-precision mode as shown below.\n\nSimilarly, specify full-precision mode on Apple M1 hardware.\n\nTo run in full-precision mode, start `dream.py` with the\n`--full_precision` flag:\n\n```\n(ldm) ~/stable-diffusion$ python scripts/dream.py --full_precision\n```\n\n# Features\n\n## **Major Features**\n\n- ## [Interactive Command Line Interface](docs/features/CLI.md)\n\n- ## [Image To Image](docs/features/IMG2IMG.md)\n\n- ## [Inpainting Support](docs/features/INPAINTING.md)\n\n- ## [GFPGAN and Real-ESRGAN Support](docs/features/UPSCALE.md)\n\n- ## [Seamless Tiling](docs/features/OTHER.md#seamless-tiling)\n\n- ## [Google Colab](docs/features/OTHER.md#google-colab)\n\n- ## [Web Server](docs/features/WEB.md)\n\n- ## [Reading Prompts From File](docs/features/OTHER.md#reading-prompts-from-a-file)\n\n- ## [Shortcut: Reusing Seeds](docs/features/OTHER.md#shortcuts-reusing-seeds)\n\n- ## [Weighted Prompts](docs/features/OTHER.md#weighted-prompts)\n\n- ## [Variations](docs/features/VARIATIONS.md)\n\n- ## [Personalizing Text-to-Image Generation](docs/features/TEXTUAL_INVERSION.md)\n\n- ## [Simplified API for text to image generation](docs/features/OTHER.md#simplified-api)\n\n## **Other Features**\n\n- ### [Creating Transparent Regions for Inpainting](docs/features/INPAINTING.md#creating-transparent-regions-for-inpainting)\n\n- ### [Preload Models](docs/features/OTHER.md#preload-models)\n\n# Latest Changes\n\n- v1.14 (11 September 2022)\n\n  - Memory optimizations for small-RAM cards. 512x512 now possible on 4 GB GPUs.\n  - Full support for Apple hardware with M1 or M2 chips.\n  - Add \"seamless mode\" for circular tiling of image. Generates beautiful effects. ([prixt](https://github.com/prixt)).\n  - Inpainting support.\n  - Improved web server GUI.\n  - Lots of code and documentation cleanups.\n\n- v1.13 (3 September 2022\n\n  - Support image variations (see [VARIATIONS](docs/features/VARIATIONS.md) ([Kevin Gibbons](https://github.com/bakkot) and many contributors and reviewers)\n  - Supports a Google Colab notebook for a standalone server running on Google hardware [Arturo Mendivil](https://github.com/artmen1516)\n  - WebUI supports GFPGAN/ESRGAN facial reconstruction and upscaling [Kevin Gibbons](https://github.com/bakkot)\n  - WebUI supports incremental display of in-progress images during generation [Kevin Gibbons](https://github.com/bakkot)\n  - A new configuration file scheme that allows new models (including upcoming stable-diffusion-v1.5)\n    to be added without altering the code. ([David Wager](https://github.com/maddavid12))\n  - Can specify --grid on dream.py command line as the default.\n  - Miscellaneous internal bug and stability fixes.\n  - Works on M1 Apple hardware.\n  - Multiple bug fixes.\n\nFor older changelogs, please visit **[CHANGELOGS](docs/CHANGELOG.md)**.\n\n# Troubleshooting\n\nPlease check out our **[Q&A](docs/help/TROUBLESHOOT.md)** to get solutions for common installation problems and other issues.\n\n# Contributing\n\nAnyone who wishes to contribute to this project, whether documentation, features, bug fixes, code cleanup, testing, or code reviews, is very much encouraged to do so. If you are unfamiliar with\nhow to contribute to GitHub projects, here is a [Getting Started Guide](https://opensource.com/article/19/7/create-pull-request-github).\n\nA full set of contribution guidelines, along with templates, are in progress, but for now the most important thing is to **make your pull request against the \"development\" branch**, and not against \"main\". This will help keep public breakage to a minimum and will allow you to propose more radical changes.\n\n## **Contributors**\n\nThis fork is a combined effort of various people from across the world. [Check out the list of all these amazing people](docs/CONTRIBUTORS.md). We thank them for their time, hard work and effort.\n\n# Support\n\nFor support,\nplease use this repository's GitHub Issues tracking service. Feel free\nto send me an email if you use and like the script.\n\nOriginal portions of the software are Copyright (c) 2020 Lincoln D. Stein (https://github.com/lstein)\n\n# Further Reading\n\nPlease see the original README for more information on this software\nand underlying algorithm, located in the file [README-CompViz.md](docs/README-CompViz.md).\n"
    },
    {
        "Repository": "Joblify",
        "Languages": "CSS, HTML, Python, ",
        "Description": "",
        "README": "# Spartahack 8 thing\n---\n\n# What Inspired This\nThe traditional job or internship search for college students involves sending similar applications to countless similar jobs. Many automated Applicant Tracking Systems (ATS) do a superficial and narrow evaluation of resumes, only looking for exact keywords, which forces students into a tedious game of \u201ckeyword optimizing\u201d their resume to \u201cbeat the bot\u201d, which shouldn\u2019t be necessary in today\u2019s world of NLP. Furthermore, employers are usually only exposed to the applicants that take the time to apply to them, instead of all possibly interested candidates.\n\nWe hope to expand and personalize the search process through a resume and job posting network, not only for job seekers but also for employers, so that they can reach out to people who are actually likely to be potential candidates. We want the job search to begin as soon as students upload their resumes.\n\n# What does it do?\nJoblify is meant for both employers and job seekers, and each group receives a different interface. When a job seeker uploads their resume, it is automatically analyzed using our NLP model and all available jobs are shown to them in order of relevance. When an employer uploads their job posting, all possible candidates are listed for them in order of qualification for the job.\n\n\n# How we built it?\n## Semantic Search Development\nThe Natural Language Processing model that powers Joblify was written using a semantic search pipeline from the spaCy library combined with a pattern using an existing job skill [dataset](https://github.com/microsoft/SkillsExtractorCognitiveSearch/tree/master/data) by Microsoft to specialize it to select job-related entities, such as fields of expertise, soft skills, and programming languages, rather than just English language entities. \n\n## Our Exploration\nOur machine learning algorithm was ultimately achieved after iterating through different machine learning models (such as whether to use named entity recognition, vector similarity, or semantic search) and libraries (including [Cohere](https://docs.cohere.ai/) and [Pinecone](https://www.pinecone.io/semantic-search/?utm_term=semantic%20search&utm_campaign=General&utm_source=adwords&utm_medium=ppc&hsa_acc=3111363649&hsa_cam=16569728073&hsa_grp=135276647740&hsa_ad=587750423757&hsa_src=g&hsa_tgt=kwd-93086337&hsa_kw=semantic%20search&hsa_mt=b&hsa_net=adwords&hsa_ver=3&gclid=EAIaIQobChMIjPuzguDs_AIVTQqtBh0SgQ-REAAYASAAEgIF7_D_BwE)), and deciding on the most performant one on our mock testing data. We used matplotlib and Plotly to visualize the effectiveness of its ability to match resumes to job descriptions.\n\n# Challenges we ran into\n1. Since we are all beginners to web development and SpartaHack8 was the first hackathon for all of us, we had hiccups with getting used to the different coding styles of each of our members and learning how to put together the different components. With help of well-defined comments, group discussions, crystal-clear concept maps on the whiteboards, and patience, we were able to develop version control and coding conventions to synergize better.\n2. Extracting entities (like skills, experiences, etc) from the job postings was a nuanced task. We started with a generally named entity recognition model from the Cohere API, but it struggled to extract relevant information. We ended up using a semantic search pipeline from spaCy in Python.\n3. Although front-end (involving HTML, CSS, and JavaScript) was the first component of our project that we got started on, at times working out the alignments of the various elements on many web pages was somewhat of a tedious task that required repetitive correction. By delegating the webpages among our 2 front-end developers, we were able to fast-track the alignment-correction process.\n\n# Accomplishments that we're proud of\nSome of the accomplishments we\u2019re most proud of include:\n\n1. We developed a fantastic looking front-end design including a brand name, logo, and core values within 14-15 hours, right from scratch\n2. We went from no knowledge of full stack development, to being able to work with flask. We \n3. Our Implementation of ML\n4. We made a prototype of the video conferencing tool with a shared whiteboard.\n\nThis 24-hour journey in which we developed Joblify had several highs and lows. The fact that we were able to enter the event with an idea, build it up bit-by-bit from scratch, create a full-stack application with frameworks we had very little experience with, and submit a project in time is a great feat for us. \n\n# What we learned\nEach of the members of our team had experience in disparate areas of development, such as front-end, machine learning, and MongoDB, but none of us had ever built a full-stack website. The hackathon offered a unique challenge for us to integrate our skills and gain exposure to how various technologies come together to create a fully usable, marketable app. With the help of mentors, tutorials, and trial and error, we learned how to combine a Javascript front-end with a back-end API using Flask. For the first time, we navigated the challenges of debugging communication between the user interface and the analysis code, as well as the database with MongoDB. All of us left with exponentially more experience in web development in Python and JavaScript than we came in with.\n\n# What's next for Joblify\n1. One of the constraints of the 24-hour hackathon is being limited to a rough prototype of the ML model without access to much user feedback to help us improve it. If we were given the time to do further research and development, using a transformer embedding model fine-tuned on training data offered by users of the site would help us greatly improve the strength and accuracy of the job and employee matching functionality.\n2. Most big employers want to sort job seekers by their ability to lead and work, and their level of experience, not just their type of skills. We will use a model that infers not just using skills, but also the level of experience and ability. \n3. Matching Employers to employees should take into consideration interest as well as expertise. A student might not be interested in a job they are very well qualified for, or vice versa.\n4. Certain qualities are more important than others. For example, time and location of availability take precedence over the job\u2019s level of relevance. Joblify should take this into account.\n\n\n\n"
    },
    {
        "Repository": "langchain-demo",
        "Languages": "Python, ",
        "Description": "",
        "README": ""
    },
    {
        "Repository": "Livestream-Tracker",
        "Languages": "JavaScript, EJS, CSS, Procfile, Shell, ",
        "Description": "An app to track the number of viewers on a Youtube Livestream over time",
        "README": "# node-js-getting-started\n\nAn app made with nodejs, Express, and EJS to track viewers \n\nThis application supports the [Getting Started on Heroku with Node.js](https://devcenter.heroku.com/articles/getting-started-with-nodejs) article - check it out.\n\n## Running Locally\n\nMake sure you have [Node.js](http://nodejs.org/) and the [Heroku CLI](https://cli.heroku.com/) installed.\n\n```sh\n$ git clone https://github.com/heroku/node-js-getting-started.git # or clone your own fork\n$ cd node-js-getting-started\n$ npm install\n$ npm start\n```\n\nYour app should now be running on [localhost:5000](http://localhost:5000/).\n\n## Deploying to Heroku\n\n```\n$ heroku create\n$ git push heroku master\n$ heroku open\n```\nor\n\n[![Deploy to Heroku](https://www.herokucdn.com/deploy/button.png)](https://heroku.com/deploy)\n\n"
    },
    {
        "Repository": "open-source-marketplace",
        "Languages": "",
        "Description": "",
        "README": ""
    },
    {
        "Repository": "playing-with-ai",
        "Languages": "Jupyter Notebook, ",
        "Description": "An unstructured collection of Colab and jupyter notebooks",
        "README": "# playing-with-ai\nAn unstructured collection of my Colab and jupyter notebooks with AI scripts and demos\n"
    },
    {
        "Repository": "roboode",
        "Languages": "Java, ",
        "Description": "Literally my first github repo",
        "README": "VM Aguments: -Xmx512M -Dsun.io.useCononCaches=false -Ddebug=true\nWorking Directory: c:\\robocode\n\nMain Class: robocode.Robocode"
    },
    {
        "Repository": "Rocket-League-Bot",
        "Languages": "Python, Batchfile, ",
        "Description": "Project for CSE 404",
        "README": "# Rocket League Bot in python\n\nUzair, Brandon, Christian, Bruce.\n\nBot that plays Rocket League, written using the RLBot framework\n\n- Requirements in `requirements.txt`\n- Bot behavior is controlled by `src/bot.py`\n- Bot appearance is controlled by `src/appearance.cfg`\n\nSee https://github.com/RLBot/RLBotPythonExample/wiki for documentation and tutorials\n"
    },
    {
        "Repository": "simple-ai-apps",
        "Languages": "Python, ",
        "Description": "testing chainlit",
        "README": ""
    },
    {
        "Repository": "simple-langchain-app",
        "Languages": "Python, ",
        "Description": "A demo app for Fall 2023 Workshop 3.",
        "README": "\r\n# Welcome to the MSU AI LangChain Demo!\r\n\r\n<!-- If you can edit this file, you're good to go! Play around with the code in the /pages folder to customize the chatbots. -->\r\n\r\n[Launch app](https://msuai-langchain-demo.streamlit.app/)\r\n"
    },
    {
        "Repository": "streamlit-demo",
        "Languages": "Jupyter Notebook, Python, ",
        "Description": "",
        "README": ""
    },
    {
        "Repository": "streamlit-workshop",
        "Languages": "Python, ",
        "Description": "Click the \"Fork\" button in the top right to create your own copy of this repository.",
        "README": "# Streamlit Workshop\n\n[Click here for workshop instructions](https://msu-ai.notion.site/Deploying-AI-Projects-with-Streamlit-b10903703adf45b6b546fc3c050cb893)\n"
    },
    {
        "Repository": "String",
        "Languages": "TypeScript, ",
        "Description": "Utility to encode/decode data in strings",
        "README": ""
    },
    {
        "Repository": "twitch-connections-graph",
        "Languages": "Python, Jupyter Notebook, ",
        "Description": "testing azure functions cd",
        "README": ""
    },
    {
        "Repository": "uzairname",
        "Languages": "",
        "Description": "Profile",
        "README": "<div align=center>\n        <a href=\"https://www.linkedin.com/in/uzair-m/\"><img src=\"https://img.shields.io/badge/Linkedin-0077b5?style=flat&logo=linkedin\" alt=\"LinkedIn\" /></a>    \n      <a href=\"mailto:uzair.m6d@gmail.com\"><img src=\"https://img.shields.io/badge/Gmail-D14836?style=for-the-badge&logo=gmail&logoColor=white\" alt=\"Gmail\" height=\"20px\" /></a> \n         <a href=\"https://github.com/uzairname\"><img src=\"https://img.shields.io/badge/-Github-000?style=flat&logo=Github&logoColor=white\" alt=\"Github\" height=\"20px\" /></a>\n</div>\n\nHi I'm Uzair Mohammed\n\nI like to learn math and experiment with AI\n\nSometimes, I make apps\n\n<h2 align=\"center\"> Stats </h2>\n<div align=\"center\"> \n  <img align=\"center\" src=\"https://github-readme-stats.vercel.app/api?username=uzairname\"/>\n<!-- [![Aman's GitHub stats](https://github-readme-stats.vercel.app/api?username=uzairname)](https://github.com/uzairname/github-readme-stats) -->\n</div> \n<div align=\"center\"> \n  <img align=\"center\" src=\"https://github-readme-stats.vercel.app/api/top-langs/?username=uzairname&layout=compact\" />\n<!-- [![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=uzairname&layout=compact)](https://github.com/uzairname/github-readme-stats) -->\n</div>\n"
    }
]